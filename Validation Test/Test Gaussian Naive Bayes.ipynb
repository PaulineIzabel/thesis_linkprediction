{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import naive_bayes\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common neighbours\n",
    "def CN(x,y,graph):\n",
    "    s = 0\n",
    "    for i in graph.neighbors(x):\n",
    "        for j in graph.neighbors(y):\n",
    "            if i == j:\n",
    "                s += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source : https://stackoverflow.com/questions/15590812/networkx-convert-multigraph-into-simple-graph-with-weighted-edges\n",
    "def multi_to_weighted(graph):\n",
    "    \n",
    "    # we create the graph with all the nodes from the original dataset\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(graph.nodes())\n",
    "    \n",
    "    for u,v,data in graph.edges(data=True):\n",
    "        w = data['weight'] if 'weight' in data else 1.0\n",
    "        if G.has_edge(u,v):\n",
    "            G[u][v]['weight'] += w\n",
    "        else:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_directed(graph, actual_edges, multi = False):\n",
    "      \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    in_deg = list(graph.in_degree)\n",
    "    out_deg = list(graph.out_degree)\n",
    "    # Clustering function does not work for multigraphs\n",
    "    if multi == True: \n",
    "        G = multi_to_weighted(graph)\n",
    "        CC = nx.clustering(G)\n",
    "    else:CC = nx.clustering(graph)\n",
    "    \n",
    "    data0_X_train = []\n",
    "    data0_Y_train = []\n",
    "    data0_nodes_train = []\n",
    "    X_train, Y_train = [],[]\n",
    "    X_test, Y_test, nodes_test = [],[],[]\n",
    "    \n",
    "    for i in range(n):\n",
    "        neighbors = [n for n in graph.neighbors(nodes[i])] # list of the neighbors of node i\n",
    "        for j in range(n):\n",
    "            if i != j :\n",
    "                \n",
    "                if nodes[j] in neighbors: \n",
    "                    features = []\n",
    "                    # there is a link between the nodes in the graph\n",
    "                    # the training set is composed of all the present edges of the graph\n",
    "                    \n",
    "                    Y_train.append(1)\n",
    "                    features.append(in_deg[i][1])\n",
    "                    features.append(in_deg[j][1])\n",
    "                    features.append(out_deg[i][1])\n",
    "                    features.append(out_deg[j][1])\n",
    "                    features.append(CC[nodes[i]])\n",
    "                    features.append(CC[nodes[j]])\n",
    "                    features.append(CN(nodes[i],nodes[j],graph))\n",
    "                    X_train.append(features)\n",
    "                \n",
    "                if (multi==False and nodes[j] not in neighbors) or (multi==True): \n",
    "                    features = []\n",
    "                    # there is no link between the nodes in the graph, if it is not a multigraph\n",
    "                    # for multigraphs, we can predict edges already existing in the graph\n",
    "                    \n",
    "                    if [nodes[i],nodes[j]] not in actual_edges: \n",
    "                        # this is not one of the removed edges\n",
    "                        data0_nodes_train.append([nodes[i],nodes[j]])\n",
    "                        data0_Y_train.append(0)\n",
    "                        features.append(in_deg[i][1])\n",
    "                        features.append(in_deg[j][1])\n",
    "                        features.append(out_deg[i][1])\n",
    "                        features.append(out_deg[j][1])\n",
    "                        features.append(CC[nodes[i]])\n",
    "                        features.append(CC[nodes[j]])\n",
    "                        features.append(CN(nodes[i],nodes[j],graph))\n",
    "                        data0_X_train.append(features)\n",
    "                    else: # the testing set is composed of all the present edges previously removed\n",
    "                        nodes_test.append([nodes[i],nodes[j]])\n",
    "                        Y_test.append(1)\n",
    "                        features.append(in_deg[i][1])\n",
    "                        features.append(in_deg[j][1])\n",
    "                        features.append(out_deg[i][1])\n",
    "                        features.append(out_deg[j][1])\n",
    "                        features.append(CC[nodes[i]])\n",
    "                        features.append(CC[nodes[j]])\n",
    "                        features.append(CN(nodes[i],nodes[j],graph))\n",
    "                        X_test.append(features)\n",
    "\n",
    "    l = int(len(data0_Y_train)/5) # the number of absent edges in the graph to remove\n",
    "   \n",
    "    # We shuffle the data of absence of edges in the same way \n",
    "    data0 = list(zip(data0_nodes_train, data0_Y_train, data0_X_train))\n",
    "    random.shuffle(data0)\n",
    "    data0_nodes_train, data0_Y_train, data0_X_train = zip(*data0)\n",
    "    \n",
    "    # We randomly remove 1/5 of the absent edges of the graph\n",
    "    X_train += data0_X_train[l:]\n",
    "    Y_train += data0_Y_train[l:]\n",
    "    \n",
    "    # We randomly add the 1/5 to the pairs of nodes to predict\n",
    "    X_test += data0_X_train[:l]\n",
    "    Y_test += data0_Y_train[:l]\n",
    "    nodes_test += data0_nodes_train[:l]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, nodes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_undirected(graph, actual_edges, multi = False):\n",
    "      \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    edges = list(graph.edges)\n",
    "    deg = list(graph.degree)\n",
    "    # Clustering function does not work for multigraphs\n",
    "    if multi == True: \n",
    "        G = multi_to_weighted(graph)\n",
    "        CC = nx.clustering(G)\n",
    "    else:CC = nx.clustering(graph)\n",
    "    \n",
    "    data0_X_train = []\n",
    "    data0_Y_train = []\n",
    "    data0_nodes_train = []\n",
    "    X_train, Y_train = [],[]\n",
    "    X_test, Y_test, nodes_test = [],[],[]\n",
    "\n",
    "    for i in range(n):\n",
    "        neighbors = [n for n in graph.neighbors(nodes[i])] # list of the neighbors of node i\n",
    "        for j in range(i+1,n):\n",
    "            if i != j :\n",
    "                \n",
    "                if nodes[j] in neighbors: \n",
    "                    features = []\n",
    "                    # there is a link between the nodes in the graph\n",
    "                    # the training set is composed of all the present edges of the graph\n",
    "                    \n",
    "                    Y_train.append(1)\n",
    "                    features.append(deg[i][1]+deg[j][1])\n",
    "                    features.append(max(deg[i][1],deg[j][1])-min(deg[i][1],deg[j][1]))\n",
    "                    features.append(CC[nodes[i]]+CC[nodes[j]])\n",
    "                    features.append(max(CC[nodes[i]],CC[nodes[j]])-min(CC[nodes[i]],CC[nodes[j]]))\n",
    "                    features.append(CN(nodes[i],nodes[j],graph))\n",
    "                    X_train.append(features)\n",
    "                \n",
    "                if (multi==False and nodes[j] not in neighbors) or (multi==True): \n",
    "                    features = []\n",
    "                    # there is no link between the nodes in the graph, if it is not a multigraph\n",
    "                    \n",
    "                    # Notation: the nodes of minimum number first                             \n",
    "                    if [min(nodes[i],nodes[j]),max(nodes[i],nodes[j])] not in actual_edges: \n",
    "                        # this is not one of the removed edges\n",
    "                        data0_nodes_train.append([min(nodes[i],nodes[j]),max(nodes[i],nodes[j])])\n",
    "                        data0_Y_train.append(0)\n",
    "                        features.append(deg[i][1]+deg[j][1])\n",
    "                        features.append(max(deg[i][1],deg[j][1])-min(deg[i][1],deg[j][1]))\n",
    "                        features.append(CC[nodes[i]]+CC[nodes[j]])\n",
    "                        features.append(max(CC[nodes[i]],CC[nodes[j]])-min(CC[nodes[i]],CC[nodes[j]]))\n",
    "                        features.append(CN(nodes[i],nodes[j],graph))\n",
    "                        data0_X_train.append(features)\n",
    "                    else: # the testing set is composed of all the present edges previously removed\n",
    "                        nodes_test.append([min(nodes[i],nodes[j]),max(nodes[i],nodes[j])])\n",
    "                        Y_test.append(1)\n",
    "                        features.append(deg[i][1]+deg[j][1])\n",
    "                        features.append(max(deg[i][1],deg[j][1])-min(deg[i][1],deg[j][1]))\n",
    "                        features.append(CC[nodes[i]]+CC[nodes[j]])\n",
    "                        features.append(max(CC[nodes[i]],CC[nodes[j]])-min(CC[nodes[i]],CC[nodes[j]]))\n",
    "                        features.append(CN(nodes[i],nodes[j],graph))\n",
    "                        X_test.append(features)         \n",
    "                \n",
    "\n",
    "    l = int(len(data0_Y_train)/5) # the number of absent edges in the graph to remove\n",
    "\n",
    "    # We shuffle the data of absence of edges in the same way \n",
    "    data0 = list(zip(data0_nodes_train, data0_Y_train, data0_X_train))\n",
    "    random.shuffle(data0)\n",
    "    data0_nodes_train, data0_Y_train, data0_X_train = zip(*data0)\n",
    "    \n",
    "    # We randomly remove 1/5 of the absent edges of the graph\n",
    "    X_train += data0_X_train[l:]\n",
    "    Y_train += data0_Y_train[l:]\n",
    "    \n",
    "    # We randomly add the 1/5 to the pairs of nodes to predict\n",
    "    X_test += data0_X_train[:l]\n",
    "    Y_test += data0_Y_train[:l]\n",
    "    nodes_test += data0_nodes_train[:l]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, nodes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(graph, actual_edges, network_type='undirected', multi = False):\n",
    "    \n",
    "    if network_type == 'directed':\n",
    "        X_train, Y_train, X_test, Y_test, nodes_test = features_directed(graph, actual_edges, multi)\n",
    "    else: \n",
    "        X_train, Y_train, X_test, Y_test, nodes_test = features_undirected(graph, actual_edges, multi)\n",
    "    NB = naive_bayes.GaussianNB()\n",
    "    NBfit = NB.fit(np.array(X_train), np.array(Y_train))\n",
    "    links = []\n",
    "    probabilities_0 = []\n",
    "    \n",
    "    for i in range(len(X_test)):  \n",
    "        prob = NBfit.predict_proba(np.array(X_test[i]).reshape(1, -1))\n",
    "        probabilities_0.append(float(prob[0][0]))\n",
    "        links.append(nodes_test[i])\n",
    "    \n",
    "    Z = sorted(zip(list(probabilities_0),links))\n",
    "    probabilities_0,links = zip(*Z)\n",
    "    return list(probabilities_0),list(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function giving the MAP of the predictions\n",
    "\n",
    "def results(probabilities_0, predictions, actual_edges, graph):\n",
    "    \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    n_edges = graph.number_of_edges()\n",
    "    test_size = len(actual_edges)\n",
    "    \n",
    "    MAP = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = len(predictions)-test_size # when 0 positive is predicted\n",
    "    fn = test_size # when 0 positive is predicted\n",
    "    stop = False # become true once we reached the threshold limit of 0.5\n",
    "    \n",
    "    \n",
    "    for k in range(len(predictions)):\n",
    "        \n",
    "        if probabilities_0[k]>0.5 and tp != 0 and stop == False:\n",
    "            # We compute MAP for the threshold 0.5\n",
    "            stop = True\n",
    "            MAP = MAP / tp\n",
    "            \n",
    "        if predictions[k] in actual_edges:\n",
    "            tp += 1\n",
    "            fn -= 1  \n",
    "            if probabilities_0[k]<=0.5:\n",
    "                MAP += (tp /(tp + fp))   \n",
    "        else:\n",
    "            fp += 1\n",
    "            tn -= 1\n",
    "          \n",
    "    if tp == 0:  # Not even one good prediction\n",
    "        return 0\n",
    "    \n",
    "    return(MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average results for the 5-CrossValidation\n",
    "\n",
    "def CV_results(list_graphs, list_targets, network_type='undirected', multi = False):\n",
    "    test_size = len(list_targets[0])\n",
    "    MAP= 0\n",
    "    \n",
    "    for part in range(5):\n",
    "        \n",
    "        probabilities_0,predictions = prediction(list_graphs[part], list_targets[part], network_type, multi)\n",
    "        \n",
    "        MAP_N = results(probabilities_0,predictions,list_targets[part],list_graphs[part])\n",
    "        MAP = MAP+MAP_N\n",
    "  \n",
    "    return(MAP/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First dataset : PROTEINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: PROTEINS\n",
    "# http://konect.uni-koblenz.de/networks/maayan-Stelzl \n",
    "\n",
    "PROT = []\n",
    "with open('out.maayan-Stelzl') as inputfile:\n",
    "    for line in inputfile:\n",
    "        PROT.append(line.strip().split(','))\n",
    "PROT = PROT[1:] # list of all the edges\n",
    "random.shuffle(PROT) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_PROT = len(PROT)\n",
    "num_vertices_PROT = 1706\n",
    "test_size_PROT = int(num_edges_PROT/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_PROT = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_PROT\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_PROT:\n",
    "        parts_PROT.append(PROT[start:])\n",
    "    else:parts_PROT.append(PROT[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_PROT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROT_G = nx.DiGraph()\n",
    "for edge in range(len(PROT)):\n",
    "    nodes = PROT[edge][0].split(' ')\n",
    "    PROT_G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "PROT_nodes = PROT_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "PROT_graphs = []\n",
    "PROT_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(PROT_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_PROT[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                target_links.append([int(nodes[0]),int(nodes[1])])\n",
    "                \n",
    "                \n",
    "\n",
    "    PROT_graphs.append(G)\n",
    "    PROT_targets.append(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03622091290062477\n"
     ]
    }
   ],
   "source": [
    "print(CV_results(PROT_graphs,PROT_targets,'directed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented algorithm: 0.036413959379415696"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second dataset: INFECTIOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: INFECTIOUS\n",
    "# http://konect.uni-koblenz.de/networks/sociopatterns-infectious \n",
    "\n",
    "INF = []\n",
    "with open('sociopatterns-infectious\\out.sociopatterns-infectious') as inputfile:\n",
    "    for line in inputfile:\n",
    "        INF.append(line.strip().split(','))\n",
    "INF = INF[2:] # list of all the edges\n",
    "random.shuffle(INF) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_INF = len(INF)\n",
    "num_vertices_INF = 410\n",
    "test_size_INF = int(num_edges_INF/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_INF = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_INF\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_INF:\n",
    "        parts_INF.append(INF[start:])\n",
    "    else:parts_INF.append(INF[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_INF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INF_G = nx.MultiGraph()\n",
    "for edge in range(len(INF)):\n",
    "    nodes = INF[edge][0].split(' ')\n",
    "    INF_G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "INF_nodes = INF_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "INF_graphs = []\n",
    "INF_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.MultiGraph()\n",
    "    G.add_nodes_from(INF_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_INF[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                # For undirected network, we write the min node first\n",
    "                # We don't want duplicates because we can only predict an edge only once\n",
    "                # We cannot predict the number of edges to add\n",
    "                if [min(int(nodes[0]),int(nodes[1])),max(int(nodes[0]),int(nodes[1]))] not in target_links:\n",
    "                    target_links.append([min(int(nodes[0]),int(nodes[1])),max(int(nodes[0]),int(nodes[1]))])\n",
    "                \n",
    "                \n",
    "    INF_graphs.append(G)\n",
    "    INF_targets.append(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8648650573701134\n"
     ]
    }
   ],
   "source": [
    "print(CV_results(INF_graphs,INF_targets,'undirected',True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented algorithm: 0.8635760390242344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third dataset: ADOLESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: ADOLESCENT\n",
    "# http://konect.uni-koblenz.de/networks/moreno_health\n",
    "\n",
    "ADO = []\n",
    "with open('moreno_health\\out.moreno_health_health') as inputfile:\n",
    "    for line in inputfile:\n",
    "        ADO.append(line.strip().split(','))\n",
    "ADO = ADO[2:] # list of all the edges\n",
    "random.shuffle(ADO) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_ADO = len(ADO)\n",
    "num_vertices_ADO = 2539\n",
    "test_size_ADO = int(num_edges_ADO/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_ADO = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_ADO\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_ADO:\n",
    "        parts_ADO.append(ADO[start:])\n",
    "    else:parts_ADO.append(ADO[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_ADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADO_G = nx.DiGraph()\n",
    "for edge in range(len(ADO)):\n",
    "    nodes = ADO[edge][0].split(' ')\n",
    "    ADO_G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2]))\n",
    "ADO_nodes = ADO_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "ADO_graphs = []\n",
    "ADO_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(ADO_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_ADO[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                target_links.append([int(nodes[0]),int(nodes[1])])\n",
    "                \n",
    "                \n",
    "\n",
    "    ADO_graphs.append(G)\n",
    "    ADO_targets.append(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21589346287544192\n"
     ]
    }
   ],
   "source": [
    "print(CV_results(ADO_graphs,ADO_targets,'directed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented algorithm: 0.20525348574968233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth dataset: MISERABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: MISERABLES\n",
    "# http://konect.uni-koblenz.de/networks/moreno_lesmis\n",
    "\n",
    "MIS = []\n",
    "with open('moreno_lesmis\\out.moreno_lesmis_lesmis') as inputfile:\n",
    "    for line in inputfile:\n",
    "        MIS.append(line.strip().split(','))\n",
    "MIS = MIS[2:] # list of all the edges\n",
    "random.shuffle(MIS) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_MIS = len(MIS)\n",
    "num_vertices_MIS = 77\n",
    "test_size_MIS = int(num_edges_MIS/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_MIS = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_MIS\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_MIS:\n",
    "        parts_MIS.append(MIS[start:])\n",
    "    else:parts_MIS.append(MIS[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_MIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIS_G = nx.Graph()\n",
    "for edge in range(len(MIS)):\n",
    "    nodes = MIS[edge][0].split(' ')\n",
    "    MIS_G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2]))\n",
    "MIS_nodes = MIS_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "MIS_graphs = []\n",
    "MIS_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(MIS_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_MIS[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                # For undirected network, we write the min node first\n",
    "                target_links.append([min(int(nodes[0]),int(nodes[1])),max(int(nodes[0]),int(nodes[1]))])\n",
    "                \n",
    "                \n",
    "\n",
    "    MIS_graphs.append(G)\n",
    "    MIS_targets.append(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8267908057765089\n"
     ]
    }
   ],
   "source": [
    "print(CV_results(MIS_graphs,MIS_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented algorithm: 0.8191563300305615"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
