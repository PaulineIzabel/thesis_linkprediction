{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random predictor for similarity-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this function to predict k links randomly\n",
    "\n",
    "def random_prediction_directed(graph, k, multi = False):\n",
    "    \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    links = []\n",
    "    random.shuffle(nodes) # so that the prediction is not defined by the order of the nodes\n",
    "    \n",
    "    while len(links) < k:\n",
    "        i = random.randint(0,n-1)\n",
    "        j = random.randint(0,n-1)\n",
    "        \n",
    "        if (multi==False and nodes[j] not in [n for n in graph.neighbors(nodes[i])]) or (multi==True): \n",
    "            # We don't want to predict edges already existing, if not multigraph\n",
    "            # if multigraph, we can predict another edge, where we already have one\n",
    "            if [nodes[i],nodes[j]] not in links : # We don't want to predict the same edge twice\n",
    "                links.append([nodes[i],nodes[j]])\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want this function to predict k links randomly\n",
    "\n",
    "def random_prediction_undirected(graph, k, multi = False):\n",
    "    \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    links = []\n",
    "    random.shuffle(nodes) # so that the prediction is not defined by the order of the nodes\n",
    "    \n",
    "    while len(links) < k:\n",
    "        i = random.randint(0,n-1)\n",
    "        j = random.randint(0,n-1)\n",
    "        \n",
    "        if (multi==False and nodes[j] not in [n for n in graph.neighbors(nodes[i])]) or (multi==True): \n",
    "            # We don't want to predict edges already existing, if not multigraph\n",
    "            # if multigraph, we can predict another edge, where we already have one\n",
    "            if [min(nodes[i],nodes[j]),max(nodes[i],nodes[j])] not in links: \n",
    "                # We don't want to predict the same edge twice\n",
    "                links.append([min(nodes[i],nodes[j]),max(nodes[i],nodes[j])])\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function giving MAP for random predictor\n",
    "\n",
    "def random_results(predictions, actual_edges, graph):\n",
    "    \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    n_edges = graph.number_of_edges()\n",
    "    test_size = len(actual_edges)\n",
    "    \n",
    "    precisions = []\n",
    "    MAP = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    \n",
    "    for k in range(len(predictions)):\n",
    "            \n",
    "        if predictions[k] in actual_edges:\n",
    "            tp += 1\n",
    "            MAP += (tp /(tp + fp))   \n",
    "        else:\n",
    "            fp += 1\n",
    "                \n",
    "    if tp == 0:  # Not even one good prediction\n",
    "        return 0\n",
    "    \n",
    "    return(MAP/tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation results for random predictor\n",
    "\n",
    "def CV_random_results(list_graphs, list_targets, network_type='undirected', multi = False):\n",
    "    test_size = len(list_targets[0])\n",
    "    MAP = 0\n",
    "    \n",
    "    for i in range(100):\n",
    "        for part in range(5):\n",
    "        \n",
    "            if network_type == 'directed':\n",
    "                predictions = random_prediction_directed(list_graphs[part],test_size,multi)\n",
    "            else: predictions = random_prediction_undirected(list_graphs[part],test_size,multi)\n",
    "            \n",
    "        MAP_N = random_results(predictions,list_targets[part],list_graphs[part])\n",
    "        MAP = MAP+MAP_N\n",
    "  \n",
    "    return(MAP/500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First dataset : PROTEINS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dataset we work on is a network of the interactions between human proteins.\n",
    "It makes a perfect example of one of the important applications of link prediction: the\n",
    "biologic domain. It is a directed, unweighted network composed of 1706 vertices\n",
    "(proteins) and 6,207 edges (interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: PROTEINS\n",
    "# http://konect.uni-koblenz.de/networks/maayan-Stelzl \n",
    "\n",
    "PROT = []\n",
    "with open('out.maayan-Stelzl') as inputfile:\n",
    "    for line in inputfile:\n",
    "        PROT.append(line.strip().split(','))\n",
    "PROT = PROT[1:] # list of all the edges\n",
    "random.shuffle(PROT) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_PROT = len(PROT)\n",
    "num_vertices_PROT = 1706\n",
    "test_size_PROT = int(num_edges_PROT/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_PROT = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_PROT\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_PROT:\n",
    "        parts_PROT.append(PROT[start:])\n",
    "    else:parts_PROT.append(PROT[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_PROT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROT_G = nx.DiGraph()\n",
    "for edge in range(len(PROT)):\n",
    "    nodes = PROT[edge][0].split(' ')\n",
    "    PROT_G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "PROT_nodes = PROT_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "PROT_graphs = []\n",
    "PROT_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(PROT_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_PROT[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                target_links.append([int(nodes[0]),int(nodes[1])])\n",
    "                \n",
    "                \n",
    "\n",
    "    PROT_graphs.append(G)\n",
    "    PROT_targets.append(target_links)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_R = CV_random_results(PROT_graphs, PROT_targets, 'directed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00035265270711450867"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second dataset: INFECTIOUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second dataset we work on is a network representing the face-to-face behaviour of\n",
    "people during the exhibition INFECTIOUS: STAY AWAY in 2009 at the Science Gallery\n",
    "in Dublin. It is undirected, unweighted but has multiple edges. It is composed of 410\n",
    "vertices (visitors) and 17,298 edges (human face-to-face)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: INFECTIOUS\n",
    "# http://konect.uni-koblenz.de/networks/sociopatterns-infectious \n",
    "\n",
    "INF = []\n",
    "with open('sociopatterns-infectious\\out.sociopatterns-infectious') as inputfile:\n",
    "    for line in inputfile:\n",
    "        INF.append(line.strip().split(','))\n",
    "INF = INF[2:] # list of all the edges\n",
    "random.shuffle(INF) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_INF = len(INF)\n",
    "num_vertices_INF = 410\n",
    "test_size_INF = int(num_edges_INF/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_INF = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_INF\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_INF:\n",
    "        parts_INF.append(INF[start:])\n",
    "    else:parts_INF.append(INF[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_INF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "INF_G = nx.MultiGraph()\n",
    "for edge in range(len(INF)):\n",
    "    nodes = INF[edge][0].split(' ')\n",
    "    INF_G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "INF_nodes = INF_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "INF_graphs = []\n",
    "INF_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.MultiGraph()\n",
    "    G.add_nodes_from(INF_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_INF[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                # For undirected network, we write the min node first\n",
    "                # We don't want duplicates because we cannot predict the number of edges to add\n",
    "                if [min(int(nodes[0]),int(nodes[1])),max(int(nodes[0]),int(nodes[1]))] not in target_links:\n",
    "                    target_links.append([min(int(nodes[0]),int(nodes[1])),max(int(nodes[0]),int(nodes[1]))])\n",
    "                \n",
    "                \n",
    "    INF_graphs.append(G)\n",
    "    INF_targets.append(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_INF_R = CV_random_results(INF_graphs, INF_targets, multi=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00422435509961273"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_INF_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third dataset: ADOLESCENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third dataset is also social related. It has been created from a survey that took place\n",
    "in 1994/1995. Each student was asked to list his 5 best female and his 5 male friends. The\n",
    "network is composed of 2,539 vertices (students), and 12,969 edges (friendships). It is\n",
    "directed and weighted. The bigger the weight of an edge is the more important the\n",
    "relation is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: ADOLESCENT\n",
    "# http://konect.uni-koblenz.de/networks/moreno_health\n",
    "\n",
    "ADO = []\n",
    "with open('moreno_health\\out.moreno_health_health') as inputfile:\n",
    "    for line in inputfile:\n",
    "        ADO.append(line.strip().split(','))\n",
    "ADO = ADO[2:] # list of all the edges\n",
    "random.shuffle(ADO) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_ADO = len(ADO)\n",
    "num_vertices_ADO = 2539\n",
    "test_size_ADO = int(num_edges_ADO/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_ADO = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_ADO\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_ADO:\n",
    "        parts_ADO.append(ADO[start:])\n",
    "    else:parts_ADO.append(ADO[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_ADO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADO_G = nx.DiGraph()\n",
    "for edge in range(len(ADO)):\n",
    "    nodes = ADO[edge][0].split(' ')\n",
    "    ADO_G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2]))\n",
    "ADO_nodes = ADO_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "ADO_graphs = []\n",
    "ADO_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(ADO_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_ADO[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                target_links.append([int(nodes[0]),int(nodes[1])])\n",
    "                \n",
    "                \n",
    "\n",
    "    ADO_graphs.append(G)\n",
    "    ADO_targets.append(target_links)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_ADO_R = CV_random_results(ADO_graphs, ADO_targets, 'directed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005662475738631031"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_ADO_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fourth dataset: MISERABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last dataset is entirely different since it is pure fiction. It contains co-occurrences of\n",
    "characters in Victor Hugo's novel Les Misérables. The network is undirected and positive\n",
    "weighted. It is composed of 77 vertices (the characters), 254 edges (co-occurrences in a\n",
    "chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: MISERABLES\n",
    "# http://konect.uni-koblenz.de/networks/moreno_lesmis\n",
    "\n",
    "MIS = []\n",
    "with open('moreno_lesmis\\out.moreno_lesmis_lesmis') as inputfile:\n",
    "    for line in inputfile:\n",
    "        MIS.append(line.strip().split(','))\n",
    "MIS = MIS[2:] # list of all the edges\n",
    "random.shuffle(MIS) # we randomly shuffle the edges\n",
    "\n",
    "# test size\n",
    "num_edges_MIS = len(MIS)\n",
    "num_vertices_MIS = 77\n",
    "test_size_MIS = int(num_edges_MIS/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "parts_MIS = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_MIS\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges_MIS:\n",
    "        parts_MIS.append(MIS[start:])\n",
    "    else:parts_MIS.append(MIS[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_MIS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIS_G = nx.Graph()\n",
    "for edge in range(len(MIS)):\n",
    "    nodes = MIS[edge][0].split(' ')\n",
    "    MIS_G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2]))\n",
    "MIS_nodes = MIS_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "MIS_graphs = []\n",
    "MIS_targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(MIS_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = parts_MIS[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1]), weight=float(nodes[2])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                # For undirected network, we write the min node first\n",
    "                target_links.append([min(int(nodes[0]),int(nodes[1])),max(int(nodes[0]),int(nodes[1]))])\n",
    "                \n",
    "                \n",
    "\n",
    "    MIS_graphs.append(G)\n",
    "    MIS_targets.append(target_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_MIS_R = CV_random_results(MIS_graphs, MIS_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013220628982735594"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_MIS_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
