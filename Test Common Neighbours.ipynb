{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Neighbour test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common neighbours\n",
    "def CN(x,y,graph):\n",
    "    s = 0\n",
    "    for i in graph.neighbors(x):\n",
    "        if i in graph.neighbors(y):\n",
    "            s += 1\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction with k the number of links we want to predict\n",
    "def prediction(graph, k):\n",
    "    \n",
    "    n = graph.number_of_nodes()\n",
    "    nodes = list(graph.nodes)\n",
    "    random.shuffle(nodes) # so that the prediction is not defined by the order of the nodes\n",
    "    links = []\n",
    "    similarities = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        neighbors = [n for n in graph.neighbors(nodes[i])] # list of the neighbors of node i\n",
    "        for j in range(i+1,n):\n",
    "            \n",
    "            if nodes[j] not in neighbors: # We don't want to predict edges already existing\n",
    "                # in the notation of the dataset the first number is always the smaller one of the vertices linked by an edge\n",
    "                links.append([min(nodes[i],nodes[j]),max(nodes[i],nodes[j])]) \n",
    "                similarities.append(CN(nodes[i],nodes[j],graph))\n",
    "    \n",
    "    Z = [x for _,x in sorted(zip(similarities,links), reverse = True)]\n",
    "    return Z[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function giving all kind of results on performance\n",
    "def results(predictions, actual_edges):\n",
    "    \n",
    "    precisions = []\n",
    "    MAP = 0\n",
    "    relevant_retrieved = 0\n",
    "    retrieved = 0\n",
    "    \n",
    "    for edge in predictions:\n",
    "        retrieved += 1\n",
    "        if edge in actual_edges:\n",
    "            relevant_retrieved += 1\n",
    "            MAP += (relevant_retrieved/retrieved)\n",
    "        precisions.append(relevant_retrieved/retrieved)\n",
    "        \n",
    "    MAP = MAP / relevant_retrieved\n",
    "    precision = precisions[len(precisions)-1]\n",
    "    recall = relevant_retrieved/len(actual_edges)\n",
    "    F1 = 2*recall*precision/(precision+recall)\n",
    "    \n",
    "    return(precisions,MAP,precision,recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: INF\n",
    "\n",
    "data = []\n",
    "with open('INF_full.net') as inputfile:\n",
    "    for line in inputfile:\n",
    "        data.append(line.strip().split(','))\n",
    "        \n",
    "# We define the number of vertices in the network\n",
    "num_vertices = int(data[0][0][10:])\n",
    "\n",
    "# We have 3 + the number of vertices lines before the lines dealing with the edges\n",
    "start_edges_INF = num_vertices+3\n",
    "INF = data[start_edges_INF:] # List of all the edges\n",
    "random.shuffle(INF) # Edges in a random order\n",
    "num_edges = len(INF)\n",
    "\n",
    "# test size\n",
    "test_size_INF = int(num_edges/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "test_INF = []\n",
    "\n",
    "start = 0\n",
    "end = test_size_INF\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges:\n",
    "        test_INF.append(INF[start:])\n",
    "    else:test_INF.append(INF[start:end])\n",
    "    start = end\n",
    "    end = start + test_size_INF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INF_G = nx.MultiGraph()\n",
    "for edge in range(len(INF)):\n",
    "    nodes = INF[edge][0].split(' ')\n",
    "    INF_G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "INF_nodes = INF_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "\n",
    "graphs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.MultiGraph()\n",
    "    G.add_nodes_from(INF_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = test_INF[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                target_links.append([int(nodes[0]),int(nodes[1])])\n",
    "                \n",
    "                \n",
    "\n",
    "    graphs.append(G)\n",
    "    targets.append(target_links)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37830018083182637\n"
     ]
    }
   ],
   "source": [
    "# Presicion\n",
    "p = 0\n",
    "for x in range(len(graphs)):\n",
    "    predictions = prediction(graphs[x],test_size_INF)\n",
    "    precisions,MAP,precision,recall,F1 = results(predictions,targets[x])\n",
    "    p = p + precision\n",
    "\n",
    "print(p/len(graphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 0.3484 We are close to the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of the dataset: HMT\n",
    "\n",
    "data = []\n",
    "with open('HMT_full.net') as inputfile:\n",
    "    for line in inputfile:\n",
    "        data.append(line.strip().split(','))\n",
    "        \n",
    "# We define the number of vertices in the network\n",
    "num_vertices = int(data[0][0][10:])\n",
    "\n",
    "# We have 3 + the number of vertices lines before the lines dealing with the edges\n",
    "start_edges_HMT = num_vertices+3\n",
    "HMT = data[start_edges_HMT:] # List of all the edges\n",
    "random.shuffle(HMT) # Edges in a random order\n",
    "num_edges = len(HMT)\n",
    "\n",
    "# test size\n",
    "test_size = int(num_edges/5)\n",
    "\n",
    "# Contains the 5 parts forming the whole dataset\n",
    "test_HMT = []\n",
    "\n",
    "start = 0\n",
    "end = test_size\n",
    "for part in range(5):  # We create the 5 parts\n",
    "    if end>num_edges:\n",
    "        test_HMT.append(HMT[start:])\n",
    "    else:test_HMT.append(HMT[start:end])\n",
    "    start = end\n",
    "    end = start + test_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HMT_G = nx.MultiGraph()\n",
    "for edge in range(len(HMT)):\n",
    "    nodes = HMT[edge][0].split(' ')\n",
    "    HMT_G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "HMT_nodes = HMT_G.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graphs contains the 5 different training networks \n",
    "# targets contains the target links corresponding\n",
    "graphs = []\n",
    "targets = []\n",
    "\n",
    "for i in range(5):     # i is the index of the folder we won't use\n",
    "    G = nx.MultiGraph()\n",
    "    G.add_nodes_from(HMT_nodes)\n",
    "    target_links = [] \n",
    "    for j in range(5): # We use every other folder\n",
    "        data = test_HMT[j]\n",
    "        if j!=i:        \n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                G.add_edge(int(nodes[0]), int(nodes[1])) \n",
    "                \n",
    "                \n",
    "        else:\n",
    "            for edge in range(len(data)):\n",
    "                nodes = data[edge][0].split(' ')\n",
    "                target_links.append([int(nodes[0]),int(nodes[1])])\n",
    "                \n",
    "                \n",
    "\n",
    "    graphs.append(G)\n",
    "    targets.append(target_links)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2696933253156945\n"
     ]
    }
   ],
   "source": [
    "# Precision\n",
    "p = 0\n",
    "for x in range(len(graphs)):\n",
    "    predictions = prediction(graphs[x],test_size)\n",
    "    precisions,MAP,precision,recall,F1 = results(predictions,targets[x])\n",
    "    p = p + precision\n",
    "\n",
    "print(p/len(graphs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 0.2453 We are close to the expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
